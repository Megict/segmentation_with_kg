{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a6bf889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe895602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import stanza\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380ec96d",
   "metadata": {},
   "source": [
    "Косинусные расстояния"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "605dcb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38f4bc3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_version = 'DeepPavlov/rubert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_version)\n",
    "model = BertModel.from_pretrained(bert_version)\n",
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f582168b",
   "metadata": {},
   "source": [
    "Средства для парсинга текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "639afd26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 11:36:10 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51e8e1b376b4694b1c97b645badba4a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 11:36:14 INFO: Loading these models for language: ru (Russian):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | syntagrus          |\n",
      "| pos       | syntagrus_charlm   |\n",
      "| lemma     | syntagrus_nocharlm |\n",
      "| depparse  | syntagrus_charlm   |\n",
      "| ner       | wikiner            |\n",
      "==================================\n",
      "\n",
      "2024-02-15 11:36:14 INFO: Using device: cpu\n",
      "2024-02-15 11:36:14 INFO: Loading: tokenize\n",
      "2024-02-15 11:36:15 INFO: Loading: pos\n",
      "2024-02-15 11:36:15 INFO: Loading: lemma\n",
      "2024-02-15 11:36:15 INFO: Loading: depparse\n",
      "2024-02-15 11:36:17 INFO: Loading: ner\n",
      "2024-02-15 11:36:23 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='ru', processors='tokenize,pos,lemma,ner,depparse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73779fc6",
   "metadata": {},
   "source": [
    "Подгрузка текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "001a52c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = {0 : {'fname' : \"text.txt\"}}\n",
    "# тут должен быть словарь с названиями файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a8d9398",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_n = 0 # задаем номер текста для обработки\n",
    "text = ''\n",
    "with io.open(texts[text_n]['fname'] , encoding='utf-8' ) as inp:\n",
    "    text += inp.read()\n",
    "    texts[text_n]['len'] = len(text)\n",
    "\n",
    "sentences = [sent for sent in sent_tokenize(text, language=\"russian\")]\n",
    "current_document = sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a839fa",
   "metadata": {},
   "source": [
    "Структуры для хранения элементов графа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c250db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subtree_:\n",
    "    def __init__(self, parse_d, text_n, sent_s):\n",
    "        self.dict = parse_d\n",
    "        \n",
    "        start_pos = None\n",
    "        end_pos = None\n",
    "        for elm in self.dict:\n",
    "            if start_pos is None or self.dict[elm]['start'] < start_pos:\n",
    "                start_pos = self.dict[elm]['start']\n",
    "            if end_pos is None or self.dict[elm]['end'] < end_pos:\n",
    "                end_pos = self.dict[elm]['end']\n",
    "        start_pos += sent_s\n",
    "        end_pos += sent_s\n",
    "        \n",
    "        self.positions = [{'t' : text_n, 's' : start_pos, 'e' : end_pos, 'm' : (start_pos + end_pos) / 2}]\n",
    "\n",
    "    def absorb(self, other):\n",
    "        # считаем, что пересечений в позициях не бывает\n",
    "        self.positions += other.positions\n",
    "\n",
    "    def print(self):\n",
    "        print(f\"=============== {self.start_pos} = {self.end_pos} ===============\")\n",
    "        for elm in self.dict:\n",
    "            print(elm, self.dict[elm])\n",
    "            \n",
    "    def dist(self, other):\n",
    "        dist = 1\n",
    "        for pos in self.positions:\n",
    "            for other_pos in other.positions:\n",
    "                if pos['t'] == other_pos['t']:\n",
    "                    dist = abs(pos['m'] - other_pos['m']) / texts[text_n]['len']\n",
    "        return dist\n",
    "    \n",
    "    def text(self):\n",
    "        text = \"\"\n",
    "        for elm in self.dict:\n",
    "            text += elm + ' '\n",
    "        return text\n",
    "    \n",
    "    def __repr__(self):\n",
    "        res = f\"{self.text()} \\t|\\t {self.positions}\" + \"\\n\"\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32f0baf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subtree(parse_d, root): # вернуть поддерево, содержащие элементы, зависящие от корня\n",
    "    subtree_d = {root : parse_d[root]}\n",
    "    while True:\n",
    "        changed = False\n",
    "        for elm in parse_d:\n",
    "            if parse_d[elm]['head'] in subtree_d.keys() and elm not in subtree_d.keys():\n",
    "                subtree_d[elm] = parse_d[elm]\n",
    "                changed = True\n",
    "        if not changed:\n",
    "            break\n",
    "            \n",
    "    return subtree_d\n",
    "\n",
    "def trim_tree(parse_d, root): # удалить из дерева все зависимости, что не являются obj и subj\n",
    "    trimmed_d = {}\n",
    "    for elm in parse_d:\n",
    "        if parse_d[elm]['dep'] in ['obj', 'subj', 'nsubj'] or elm == root:\n",
    "            trimmed_d[elm] = parse_d[elm]\n",
    "            \n",
    "    return trimmed_d            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "293b3e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class graph_base:\n",
    "    def __init__(self, subtrees_list, dist_measure = calc_weight):\n",
    "        self.nodes_list = subtrees_list\n",
    "        self.nodes = {subtree.text() : subtree for subtree in subtrees_list}\n",
    "        self.dist_measure = dist_measure\n",
    "\n",
    "    def find_weight_txt(self, txt_1, txt_2):\n",
    "        return self.dist_measure(self.nodes[txt_1],self.nodes[txt_2])\n",
    "        \n",
    "    def find_weight_pos(self, num_1, num_2):\n",
    "        return self.dist_measure(self.nodes_list[num_1],self.nodes_list[num_2])\n",
    "    \n",
    "    def find_all_distances(self, elm_pos):\n",
    "        dist_list = {}\n",
    "        for pos in range(len(self.nodes_list)):\n",
    "            dist_list[self.nodes_list[pos].text()] = self.find_weight_pos(elm_pos, pos)\n",
    "        return dist_list\n",
    "    \n",
    "    def find_closest(self, elm_pos, amount = 1):\n",
    "        dist_list = {}\n",
    "        for pos in range(len(self.nodes_list)):\n",
    "            dist_list[self.nodes_list[pos].text()] = self.find_weight_pos(elm_pos, pos)\n",
    "\n",
    "        return list(reversed(sorted(dist_list.items(), key=lambda item: item[1])))[0 : amount]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040cf7d4",
   "metadata": {},
   "source": [
    "Функции для измерения весов ребер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9b9d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shanon_entropy(parse_d):\n",
    "    subtree_str = ''\n",
    "    for elm in parse_d:\n",
    "        subtree_str += ' ' + elm\n",
    "    str_elements = set(subtree_str)\n",
    "    entropy = 0\n",
    "    for elm in str_elements:\n",
    "        prob = subtree_str.count(elm) / len(subtree_str)\n",
    "        entropy -= prob * math.log2(prob)\n",
    "        \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23b023a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(sent_1, sent_2):\n",
    "    texts = [sent_1, sent_2]\n",
    "    encodings = tokenizer(texts, padding=True, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        embed_1, embed_2 = model(**encodings)[0].cpu()\n",
    "    return cosine_similarity(embed_1, embed_2).mean()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8fe4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_weight(lhs : Subtree_, rhs : Subtree_):\n",
    "    return 1 - lhs.dist(rhs) + calculate_cosine_similarity(lhs.text(), rhs.text()) + shanon_entropy(rhs.text()) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561ca571",
   "metadata": {},
   "source": [
    "Собственно, парсинг текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "666d23d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 244/244 [04:26<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# добавление документа к графу\n",
    "selected_subtrees = []\n",
    "sent_start = 0\n",
    "\n",
    "current_document_num = 0 #my_knowledge_graph.lst_doc_index + 1\n",
    "current_document_graph = nx.DiGraph() # граф, который будет строиться во время обработки документа, а затем сольется с исходным графом\n",
    "# альтернативный подъод - искать в сузествующем графе слова из обрабатываемого документа, если их нет, то делать новые вершины прям в нем\n",
    "# но мы это и делаем при слиянии (по крайней мере планируем делать)\n",
    "\n",
    "current_sentence_num = -1\n",
    "# проход по предложениям в документе\n",
    "for s in tqdm(current_document):\n",
    "    current_sentence_num += 1\n",
    "\n",
    "    doc = nlp(s)\n",
    "    sent = doc.sentences[0]\n",
    "\n",
    "    temp_d = dict()\n",
    "    \n",
    "    # преобразование вывода пайплайна в словарь, содержащий только необходимые ключи\n",
    "    current_sentence_pos = -1\n",
    "    for word in sent.words:\n",
    "        current_sentence_pos += 1\n",
    "        temp_d[word.text] = {\"head\": sent.words[word.head-1].text, \n",
    "                             \"dep\": word.deprel, \n",
    "                             \"id\": word.id, \n",
    "                             \"upos\": word.upos,\n",
    "                             \"lem\": word.lemma.lower(),\n",
    "                             \"start\" : word.start_char,\n",
    "                             \"end\" : word.end_char}\n",
    "    \n",
    "    \n",
    "    cur_sent_len = temp_d[list(temp_d.keys())[-1]]['end']\n",
    "    \n",
    "    for elm in temp_d.keys():\n",
    "        sent_start += len(elm) + 1\n",
    "        # определяем, делаем ли мы из этого вершины и ребра\n",
    "        if     temp_d[elm]['upos'] == 'NOUN'\\\n",
    "            or temp_d[elm]['upos'] == 'NUM':\n",
    "            \n",
    "            sub_d = extract_subtree(temp_d, elm)\n",
    "            trimmed_sub_d = trim_tree(sub_d, elm)\n",
    "            subtree = Subtree_(trimmed_sub_d, 0, sent_start)\n",
    "            \n",
    "            selected_subtrees.append(subtree)\n",
    "           \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd4be63",
   "metadata": {},
   "source": [
    "Тут убираются дубликаты, созраняются списки упоминаний каждого уникального элемента знаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d946bab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_subtrees = []\n",
    "absorbed_ids = []\n",
    "for ind in range(len(selected_subtrees)):\n",
    "    if ind in absorbed_ids:\n",
    "        continue\n",
    "    for other_ind in range(ind + 1, len(selected_subtrees)):\n",
    "        if selected_subtrees[ind].text() == selected_subtrees[other_ind].text():\n",
    "            selected_subtrees[ind].absorb(selected_subtrees[other_ind])\n",
    "            absorbed_ids.append(other_ind)\n",
    "    merged_subtrees.append(selected_subtrees[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74622af",
   "metadata": {},
   "source": [
    "Собатвенно, пример работы с графом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f457f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_base(merged_subtrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa95777a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "лекции  \t|\t [{'t': 0, 's': 3315, 'e': 3321, 'm': 3318.0}]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('сожалению ', 2.5114902473806398),\n",
       " ('изображении ', 2.474484403074112),\n",
       " ('учреждения ', 2.460029080098655),\n",
       " ('роста Первый очки шляпу ', 2.4575713087864135)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 100\n",
    "print(graph.nodes_list[n])\n",
    "graph.find_closest(n,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d020176b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c8b292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
